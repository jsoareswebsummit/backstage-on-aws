{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Backstage Infrastructure This project uses CDK to deploy a containerized version of Backstage running in AWS ECS Fargate with AWS Aurora Postgres, along with all the other required supporting infrastructure. This enables a serverless deployment of your Backstage application. This CDK project creates two independent stacks which create or deploy CI/CD pipelines, supporting application infrastructure, and app containers to AWS. The combination of pipelines, multiple environments, and cdk enables a continuous deployment workflow with release on demand capabilities. The two stacks each have important roles: The backstage stack and pipeline This stack creates multiple environments within a ECS Fargate cluster along with multiple Aurora postgres dbs for persistence, and puts those behind application load balancers each with a custom domain name. Additionally it builds a codepipeline called the backstage-app-pipline which builds and deploys a new application image based on commits in a repo where the backstage app code resides via an ECS deployment event. Once this cdk stack is deployed, you only need to update your backstage application code to update the current running application. By using multiple stages, you are able to test your new changes before manually promoting to the production environment. The infrastructure pipeline The infra pipeline stack creates a codepipeline to both update itself and to deploy the backstage stack described above. This set of infrastructure code is self-updating and self-healing, and requires no manual intervention after the initial manual deployment of the infra-pipeline stack. Diagram Documentation Pipelines Descriptions Application Infrstructure Settings and Configuration Bootstrap the Stacks Deployment","title":"Introduction"},{"location":"#backstage-infrastructure","text":"This project uses CDK to deploy a containerized version of Backstage running in AWS ECS Fargate with AWS Aurora Postgres, along with all the other required supporting infrastructure. This enables a serverless deployment of your Backstage application. This CDK project creates two independent stacks which create or deploy CI/CD pipelines, supporting application infrastructure, and app containers to AWS. The combination of pipelines, multiple environments, and cdk enables a continuous deployment workflow with release on demand capabilities. The two stacks each have important roles:","title":"Backstage Infrastructure"},{"location":"#the-backstage-stack-and-pipeline","text":"This stack creates multiple environments within a ECS Fargate cluster along with multiple Aurora postgres dbs for persistence, and puts those behind application load balancers each with a custom domain name. Additionally it builds a codepipeline called the backstage-app-pipline which builds and deploys a new application image based on commits in a repo where the backstage app code resides via an ECS deployment event. Once this cdk stack is deployed, you only need to update your backstage application code to update the current running application. By using multiple stages, you are able to test your new changes before manually promoting to the production environment.","title":"The backstage stack and pipeline"},{"location":"#the-infrastructure-pipeline","text":"The infra pipeline stack creates a codepipeline to both update itself and to deploy the backstage stack described above. This set of infrastructure code is self-updating and self-healing, and requires no manual intervention after the initial manual deployment of the infra-pipeline stack.","title":"The infrastructure pipeline"},{"location":"#diagram","text":"","title":"Diagram"},{"location":"#documentation","text":"Pipelines Descriptions Application Infrstructure Settings and Configuration Bootstrap the Stacks Deployment","title":"Documentation"},{"location":"bootstrap/","text":"Bootstrapping and Setup There is a bit of manual bootstrapping in the deployment account required for the pipelines and stacks to succeed. This maybe mitigated with some automation scripting and added features to the cdk stacks. Table of Contents Create a Route53 hosted zones Create ACM certs (optional) Create Github OAuth Tokens Create AWS integration User and Tokens Store a secrets in AWS Secrets Manager Store Github-App secrets file(s) in Secrets Manager. Create ECR repositories Codestar Connections and Notifications Create a Route53 hosted zones The cdk stack assumes a pre-existing domain name and hosted zone in route53 for each stage. You must create manually a subdomain for each of the stages specified in route53. The stack will then associate that domain name with the application load balancer for that particular stage. Create ACM certs (optional) If the stack does not find ACM_ARN defined in either the common configuration or the stage configurations it will try to create an ACM with the domain name specified for the stage. Create Github OAuth Tokens If you want to use Github auth as your backstage IDP, you will need to create an Oauth token to allow backstage to connect. This can only be done by an owner of the org, and is found under the org developer settings. Save the token id and secret for storage in AWS secrets manager. Each OAuth token depends on callbacks to the url for the application and so must be unique for each domain name of the application. So if you have multiple stages configured, you will need to create multiple OAuth applications and secrets. Create AWS integration User and Tokens The backstage app uses tokens for access to AWS services like techdocs as well as any aws based plugin, in theory it should be able to use the credentials attached to the ECS container via a role (similar to EC2 roles), but this is a recent development and hasnt been tested. Currently we use setting particular environment variables in the runtime environment of the container to give the sevice access to AWS resources. We created an integration user and group and attached policies to the group for that user. The access token and secret are stored in AWS Secrets Manager. The current polices allowed for the group we used are: - ReadOnlyAccess Store a secrets in AWS Secrets Manager To give the backstage services access to 3rd party apis securely, we bootstrap aws secrets manager with the tokens created above and extract them at deploy time to inject in the infrastructure as environment variables You will want to add the tokens created for Github auth and AWS auth into secret manager. The cdk stack will lookup those secrets and create new env vars detailed below and inject them into the container at runtime as the following variables: AUTH_GITHUB_CLIENT_ID --> looks in the secret name stored in GITHUB_AUTH_SECRET_NAME for a key name of 'id' AUTH_GITHUB_CLIENT_SECRET --> looks in the secret name stored in GITHUB_AUTH_SECRET_NAME for a key name of 'secret' AWS_ACCESS_KEY_ID --> looks in the secret name stored in AWS_AUTH_SECRET_NAME for a key name of 'id' AWS_ACCESS_KEY_SECRET --> looks in the secret name stored in AWS_AUTH_SECRET_NAME for a key name of 'secret' So we need to create secrets in secret manager with the following fields - Key: 'id' - Value: token id - Key: 'secret' - Value: token secret Further because the test instance and prod instance each have unique domain names we need separate github auth secrets configured for each environment. See env-config.yaml , for examples of the github auth for each stage. Store Github-App secrets file(s) in Secrets Manager. Instead of using personal access tokens for backstage access the github apis, we have configured the use of a github-app for each of the orgs in github that you want to connect to. Use of these apps requires providing a crendential configuration file to backstage at run time. The file includes a full PEM certificate that makes it difficult to pass via environment variables, so we store the raw file in Secrets Manager ( see example ) retrieve the contents in the app pipeline and write that to a file which is copied into the image during build. The ARNs for those secrets are loaded from the env-config.yaml file. To create a new github-app you can use a utility in the backstage app with yarn backstage-cli create-github-app <github-org> some permissions will need to be updated in Github on the app for the backstage to be able to execute all its actions, you can find those permissions under settings > developer settings > github-app. The following permissions are currently set in both orgs: actions (r/w) administration (r/w) contents (r/w) issues (r/w) metadata (r) pages (r/w) pull-requests (r/w) workflows (r/w) members (r) email addresses (r) Create ECR repositories The application pipeline will build and push to an ECR repository name in the ECR_REPO_NAME variable in env-config.yaml . The cdk stack will attempt to create this repo with the name set or with the CONTAINER_NAME . The app-pipeline will build the first image and push it succesfully, so you do not need to pre-build and pre-seed the repo with an image. Codestar Connections and Notifications Both pipelines use a codestar connection to authenticate to github to watch for changes, as well as a codestar notification to push notifcations to slack via the AWS Chatbot integration. Each of these need to be setup in advance, at the time CDK did not support creation of these. The configuration file just needs the arn of each to provide to CDK stacks.","title":"Boostrap and Setup"},{"location":"bootstrap/#bootstrapping-and-setup","text":"There is a bit of manual bootstrapping in the deployment account required for the pipelines and stacks to succeed. This maybe mitigated with some automation scripting and added features to the cdk stacks.","title":"Bootstrapping and Setup"},{"location":"bootstrap/#table-of-contents","text":"Create a Route53 hosted zones Create ACM certs (optional) Create Github OAuth Tokens Create AWS integration User and Tokens Store a secrets in AWS Secrets Manager Store Github-App secrets file(s) in Secrets Manager. Create ECR repositories Codestar Connections and Notifications","title":"Table of Contents"},{"location":"bootstrap/#create-a-route53-hosted-zones","text":"The cdk stack assumes a pre-existing domain name and hosted zone in route53 for each stage. You must create manually a subdomain for each of the stages specified in route53. The stack will then associate that domain name with the application load balancer for that particular stage.","title":"Create a Route53 hosted zones"},{"location":"bootstrap/#create-acm-certs-optional","text":"If the stack does not find ACM_ARN defined in either the common configuration or the stage configurations it will try to create an ACM with the domain name specified for the stage.","title":"Create ACM certs (optional)"},{"location":"bootstrap/#create-github-oauth-tokens","text":"If you want to use Github auth as your backstage IDP, you will need to create an Oauth token to allow backstage to connect. This can only be done by an owner of the org, and is found under the org developer settings. Save the token id and secret for storage in AWS secrets manager. Each OAuth token depends on callbacks to the url for the application and so must be unique for each domain name of the application. So if you have multiple stages configured, you will need to create multiple OAuth applications and secrets.","title":"Create Github OAuth Tokens"},{"location":"bootstrap/#create-aws-integration-user-and-tokens","text":"The backstage app uses tokens for access to AWS services like techdocs as well as any aws based plugin, in theory it should be able to use the credentials attached to the ECS container via a role (similar to EC2 roles), but this is a recent development and hasnt been tested. Currently we use setting particular environment variables in the runtime environment of the container to give the sevice access to AWS resources. We created an integration user and group and attached policies to the group for that user. The access token and secret are stored in AWS Secrets Manager. The current polices allowed for the group we used are: - ReadOnlyAccess","title":"Create AWS integration User and Tokens"},{"location":"bootstrap/#store-a-secrets-in-aws-secrets-manager","text":"To give the backstage services access to 3rd party apis securely, we bootstrap aws secrets manager with the tokens created above and extract them at deploy time to inject in the infrastructure as environment variables You will want to add the tokens created for Github auth and AWS auth into secret manager. The cdk stack will lookup those secrets and create new env vars detailed below and inject them into the container at runtime as the following variables: AUTH_GITHUB_CLIENT_ID --> looks in the secret name stored in GITHUB_AUTH_SECRET_NAME for a key name of 'id' AUTH_GITHUB_CLIENT_SECRET --> looks in the secret name stored in GITHUB_AUTH_SECRET_NAME for a key name of 'secret' AWS_ACCESS_KEY_ID --> looks in the secret name stored in AWS_AUTH_SECRET_NAME for a key name of 'id' AWS_ACCESS_KEY_SECRET --> looks in the secret name stored in AWS_AUTH_SECRET_NAME for a key name of 'secret' So we need to create secrets in secret manager with the following fields - Key: 'id' - Value: token id - Key: 'secret' - Value: token secret Further because the test instance and prod instance each have unique domain names we need separate github auth secrets configured for each environment. See env-config.yaml , for examples of the github auth for each stage.","title":"Store a secrets in AWS Secrets Manager"},{"location":"bootstrap/#store-github-app-secrets-files-in-secrets-manager","text":"Instead of using personal access tokens for backstage access the github apis, we have configured the use of a github-app for each of the orgs in github that you want to connect to. Use of these apps requires providing a crendential configuration file to backstage at run time. The file includes a full PEM certificate that makes it difficult to pass via environment variables, so we store the raw file in Secrets Manager ( see example ) retrieve the contents in the app pipeline and write that to a file which is copied into the image during build. The ARNs for those secrets are loaded from the env-config.yaml file. To create a new github-app you can use a utility in the backstage app with yarn backstage-cli create-github-app <github-org> some permissions will need to be updated in Github on the app for the backstage to be able to execute all its actions, you can find those permissions under settings > developer settings > github-app. The following permissions are currently set in both orgs: actions (r/w) administration (r/w) contents (r/w) issues (r/w) metadata (r) pages (r/w) pull-requests (r/w) workflows (r/w) members (r) email addresses (r)","title":"Store Github-App secrets file(s) in Secrets Manager."},{"location":"bootstrap/#create-ecr-repositories","text":"The application pipeline will build and push to an ECR repository name in the ECR_REPO_NAME variable in env-config.yaml . The cdk stack will attempt to create this repo with the name set or with the CONTAINER_NAME . The app-pipeline will build the first image and push it succesfully, so you do not need to pre-build and pre-seed the repo with an image.","title":"Create ECR repositories"},{"location":"bootstrap/#codestar-connections-and-notifications","text":"Both pipelines use a codestar connection to authenticate to github to watch for changes, as well as a codestar notification to push notifcations to slack via the AWS Chatbot integration. Each of these need to be setup in advance, at the time CDK did not support creation of these. The configuration file just needs the arn of each to provide to CDK stacks.","title":"Codestar Connections and Notifications"},{"location":"deploy/","text":"Initialize and Deploy CDK project This project is set up like a standard Python CDK project. The initialization process also creates a virtualenv within this project, stored under the .venv directory. To create the virtualenv it assumes that there is a python3 executable in your path with access to the venv package. To manually create a virtualenv on MacOS and Linux: $ python3 -m venv .venv After the init process completes and the virtualenv is created, you can use the following step to activate your virtualenv. $ source .venv/bin/activate Once the virtualenv is activated, you can install the required dependencies. $ pip install -r requirements.txt At this point you can now synthesize the CloudFormation template for this code. $ cdk synth Finally, assuming no errors from synth, you have credentials to deploy to the account you wish, and you have set your env vars in a env-config.yaml file, you can deploy the infrastructure pipeline. Note: the infrastructure pipeline will build itself, then the backstage-infra stack including the application pipeline. It wont be until the app pipeline completes its first pass that a running task will be available in Fargate. $ cdk deploy backstage-infra-pipeline Now sit back, grab some coffee, and watch Cloudformation and codepipeline work as your infrastructure and backstage app come to life! Enjoy!","title":"Deployment"},{"location":"deploy/#initialize-and-deploy-cdk-project","text":"This project is set up like a standard Python CDK project. The initialization process also creates a virtualenv within this project, stored under the .venv directory. To create the virtualenv it assumes that there is a python3 executable in your path with access to the venv package. To manually create a virtualenv on MacOS and Linux: $ python3 -m venv .venv After the init process completes and the virtualenv is created, you can use the following step to activate your virtualenv. $ source .venv/bin/activate Once the virtualenv is activated, you can install the required dependencies. $ pip install -r requirements.txt At this point you can now synthesize the CloudFormation template for this code. $ cdk synth Finally, assuming no errors from synth, you have credentials to deploy to the account you wish, and you have set your env vars in a env-config.yaml file, you can deploy the infrastructure pipeline. Note: the infrastructure pipeline will build itself, then the backstage-infra stack including the application pipeline. It wont be until the app pipeline completes its first pass that a running task will be available in Fargate. $ cdk deploy backstage-infra-pipeline Now sit back, grab some coffee, and watch Cloudformation and codepipeline work as your infrastructure and backstage app come to life! Enjoy!","title":"Initialize and Deploy CDK project"},{"location":"pipelines/","text":"Pipelines This repo has CDK stacks which creates two separate pipelines for the backstage deployment. Each pipeline has a different source and controls a different aspect of the backstage deployment. Infrastructure Pipeline Application Pipeline Diagram of pipelines and stacks Infrastructure Pipeline The infrastructure pipeline is the only part of the deployment that requires an inital manual deployment. This pipeline is self updating and deploys all of the supporting infrastructure to host the backstage container app as well as deploys the application pipeline detailed below. This pipeline uses the infra-buildspec.yaml to specify the actions for its codebuild stage, which synthesizes the cdk stacks into cloudformation templates. The subsequent deployment stages, one for each stack, then use a create/replace changeset action to deploy the cloudformation changes to the deployment account. Application Pipeline The application pipeline is created in the backstage-infra stack deployed by the infrastructure pipeline. This pipeline is only intended to build and update the containers running in ECS fargate based on changes in the application code, so that we can maintain the two aspects separately and cleanly. This way the application code is not required to contain information about its deployment infrastructure. This allows us to cleanly test and develop the applicaton code locally. The app pipeline uses a codebuild stage and the app-buildspec.yml file to perform the docker image build, and push the latest image to the ECR repository. We try to reduce the build time by pulling and specifying cached images for the build. The subsequent stages then use the ECS Deploy action to create an updated imagedefinitions.json file and notify the ECS service and task definitions of a pending change in container versions. ECS then takes over to cleanly swap out running containers in each of the stage tasks, so no downtime occurs. There is an optional manual approval action inserted before the deployment for prod stage, which allows us to check the test stage before promoting.","title":"Pipelines Description"},{"location":"pipelines/#pipelines","text":"This repo has CDK stacks which creates two separate pipelines for the backstage deployment. Each pipeline has a different source and controls a different aspect of the backstage deployment. Infrastructure Pipeline Application Pipeline","title":"Pipelines"},{"location":"pipelines/#diagram-of-pipelines-and-stacks","text":"","title":"Diagram of pipelines and stacks"},{"location":"pipelines/#infrastructure-pipeline","text":"The infrastructure pipeline is the only part of the deployment that requires an inital manual deployment. This pipeline is self updating and deploys all of the supporting infrastructure to host the backstage container app as well as deploys the application pipeline detailed below. This pipeline uses the infra-buildspec.yaml to specify the actions for its codebuild stage, which synthesizes the cdk stacks into cloudformation templates. The subsequent deployment stages, one for each stack, then use a create/replace changeset action to deploy the cloudformation changes to the deployment account.","title":"Infrastructure Pipeline"},{"location":"pipelines/#application-pipeline","text":"The application pipeline is created in the backstage-infra stack deployed by the infrastructure pipeline. This pipeline is only intended to build and update the containers running in ECS fargate based on changes in the application code, so that we can maintain the two aspects separately and cleanly. This way the application code is not required to contain information about its deployment infrastructure. This allows us to cleanly test and develop the applicaton code locally. The app pipeline uses a codebuild stage and the app-buildspec.yml file to perform the docker image build, and push the latest image to the ECR repository. We try to reduce the build time by pulling and specifying cached images for the build. The subsequent stages then use the ECS Deploy action to create an updated imagedefinitions.json file and notify the ECS service and task definitions of a pending change in container versions. ECS then takes over to cleanly swap out running containers in each of the stage tasks, so no downtime occurs. There is an optional manual approval action inserted before the deployment for prod stage, which allows us to check the test stage before promoting.","title":"Application Pipeline"},{"location":"settings/","text":"Settings and Configurations Create a env-config.yaml file Create a yaml file env-config.yaml in the config directory of this project with your secrets names and parameters to configure both the CDK deployment and to pass them to your backstage app container at runtime. A full clean and separate replicaton of the stacks and pipelines can be deployed with a new configuration file, by changing out the name of the the configuration file in app.py . This allows us to do a pre-test of any major changes to infrastructure or deployment flow in another account. see: env-config-test.yaml Below are the variables used by cdk, you may add any others to env-config.yaml that you want to pass to the backstage runtime as env variables. The essential variables for CDK deployment to define are: Postgres config (Optional) POSTGRES_PORT --> (Optional) defaults to 5432 POSTGRES_DB --> (Optional) no default lets backstage define POSTGRES_USER --> (Optional) defaults to 'postgres' POSTGRES_PASSWORD --> (Optional) Not needed, will get generated and over-written on the fly POSTGRES_HOST --> (Optional) Not needed, will get generated and over-written on the fly Routing & Discovery HOST_NAME --> (Required) defaults to backstage, must be unique for each stage DOMAIN_NAME --> (Required) defaults to example.com CONTAINER_PORT --> (Optional) defaults to 7000 CONTAINER_NAME --> (Optional) defaults to 'Backstage' DOCKERFILE --> (Optional) defaults to 'dockerfile' AWS Environment AWS_REGION --> (Optional) defaults to 'us-east-1' AWS_ACCOUNT --> (Required) no default ACM_ARN --> (Optional) no default, if left empty will generate a new cert ECR_REP_NAME --> (Optional) if not set cdk will create new repo with container name. TAG_STACK_NAME -> (Required) no default TAG_STACK_PRODUCT -> (Optional) defaults to \"Dev-Portal\" Github Repo Info for Pipeline GITHUB_APP_REPO --> (Required) the name of the repo where your backstage app code is kept, without the user or org GITHUB_INFRA_REPO --> (Required) the name of this repo GITHUB_ORG --> (Required) the user or org where the backstage app repo lives GITHUB_INFRA_BRANCH --> (Optional) defaults to \"main\" GITHUB_APP_BRANCH --> (Optional) defaults to \"main\" CODESTAR_CONN_ARN --> (Required) the bootstrapped codestar connection for the pipelines to use CODESTAR_NOTIFY_ARN --> (Optional) the codestar notification connection for chatbot ARN Secrets to retreive at runtime but keep hidden: AWS_AUTH_SECRET_NAME --> (Required) the name of the AWS Secretmanager Secret which contains key/secret for backstage to acces aws services GITHUB_AUTH_SECRET_NAME --> (Required per stage) the name of the AWS Secretmanager Secret which holds the oauth key/secret for backstage to auth against Github GITHUB_APP_ARN --> (Required) secret arn for github-app file, used in the app-pipeline during container build.","title":"Settings and Config"},{"location":"settings/#settings-and-configurations","text":"","title":"Settings and Configurations"},{"location":"settings/#create-a-env-configyaml-file","text":"Create a yaml file env-config.yaml in the config directory of this project with your secrets names and parameters to configure both the CDK deployment and to pass them to your backstage app container at runtime. A full clean and separate replicaton of the stacks and pipelines can be deployed with a new configuration file, by changing out the name of the the configuration file in app.py . This allows us to do a pre-test of any major changes to infrastructure or deployment flow in another account. see: env-config-test.yaml Below are the variables used by cdk, you may add any others to env-config.yaml that you want to pass to the backstage runtime as env variables. The essential variables for CDK deployment to define are:","title":"Create a env-config.yaml file"},{"location":"settings/#postgres-config-optional","text":"POSTGRES_PORT --> (Optional) defaults to 5432 POSTGRES_DB --> (Optional) no default lets backstage define POSTGRES_USER --> (Optional) defaults to 'postgres' POSTGRES_PASSWORD --> (Optional) Not needed, will get generated and over-written on the fly POSTGRES_HOST --> (Optional) Not needed, will get generated and over-written on the fly","title":"Postgres config (Optional)"},{"location":"settings/#routing-discovery","text":"HOST_NAME --> (Required) defaults to backstage, must be unique for each stage DOMAIN_NAME --> (Required) defaults to example.com CONTAINER_PORT --> (Optional) defaults to 7000 CONTAINER_NAME --> (Optional) defaults to 'Backstage' DOCKERFILE --> (Optional) defaults to 'dockerfile'","title":"Routing &amp; Discovery"},{"location":"settings/#aws-environment","text":"AWS_REGION --> (Optional) defaults to 'us-east-1' AWS_ACCOUNT --> (Required) no default ACM_ARN --> (Optional) no default, if left empty will generate a new cert ECR_REP_NAME --> (Optional) if not set cdk will create new repo with container name. TAG_STACK_NAME -> (Required) no default TAG_STACK_PRODUCT -> (Optional) defaults to \"Dev-Portal\"","title":"AWS Environment"},{"location":"settings/#github-repo-info-for-pipeline","text":"GITHUB_APP_REPO --> (Required) the name of the repo where your backstage app code is kept, without the user or org GITHUB_INFRA_REPO --> (Required) the name of this repo GITHUB_ORG --> (Required) the user or org where the backstage app repo lives GITHUB_INFRA_BRANCH --> (Optional) defaults to \"main\" GITHUB_APP_BRANCH --> (Optional) defaults to \"main\" CODESTAR_CONN_ARN --> (Required) the bootstrapped codestar connection for the pipelines to use CODESTAR_NOTIFY_ARN --> (Optional) the codestar notification connection for chatbot ARN","title":"Github Repo Info for Pipeline"},{"location":"settings/#secrets-to-retreive-at-runtime-but-keep-hidden","text":"AWS_AUTH_SECRET_NAME --> (Required) the name of the AWS Secretmanager Secret which contains key/secret for backstage to acces aws services GITHUB_AUTH_SECRET_NAME --> (Required per stage) the name of the AWS Secretmanager Secret which holds the oauth key/secret for backstage to auth against Github GITHUB_APP_ARN --> (Required) secret arn for github-app file, used in the app-pipeline during container build.","title":"Secrets to retreive at runtime but keep hidden:"},{"location":"stack/","text":"Backstage Infrastructure Stack The infrastructure stack to host backstage consists of: ECS Fargate cluster ECS Service definition ECS Task definitions for each stage (Test, Prod) Aurora postgresql dbs for each stage (Test, Prod) Public and Private subnets on a dedicated VPC Elastic Load Balancers for each stage (Test, Prod) ACM Certs for each stage (Test, Prod) Application Pipeline to build and deploy the application code in a container This stack is created and deployed by the infrastructure pipeline. Multiple stages can be added by adding more stages to the env-config.yaml file.","title":"Application Infrastructure"},{"location":"stack/#backstage-infrastructure-stack","text":"The infrastructure stack to host backstage consists of: ECS Fargate cluster ECS Service definition ECS Task definitions for each stage (Test, Prod) Aurora postgresql dbs for each stage (Test, Prod) Public and Private subnets on a dedicated VPC Elastic Load Balancers for each stage (Test, Prod) ACM Certs for each stage (Test, Prod) Application Pipeline to build and deploy the application code in a container This stack is created and deployed by the infrastructure pipeline. Multiple stages can be added by adding more stages to the env-config.yaml file.","title":"Backstage Infrastructure Stack"}]}